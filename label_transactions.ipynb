{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"transactions_training_data.csv\",parse_dates=[\"Date\"]).query(\"Date>='2017-07-01'\")\n",
    "sep = \" \"\n",
    "data[\"feature_string\"] = \\\n",
    "(data.Category + sep + \\\n",
    "data.Date.dt.day_name() + sep + \\\n",
    "data[\"Account Name\"] + sep + \\\n",
    "data[\"Transaction Type\"] + sep + \\\n",
    "data.Description).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"label\"] = ~data.Labels.isna()\n",
    "data[\"feature_float\"] = data.Amount\n",
    "data[\"weights\"] = np.ones_like(data.Amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<youtokentome.youtokentome.BPE at 0x7fd23054dd10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import youtokentome as yttm\n",
    "\n",
    "train_data_path = \"train_data.txt\"\n",
    "model_path = \"vocab.model\"\n",
    "\n",
    "open(train_data_path,\"w\").write(\"\\n\".join(data.feature_string.values))\n",
    "\n",
    "yttm.BPE.train(data=train_data_path, vocab_size=1000, model=model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = yttm.BPE(model=model_path)\n",
    "features_ids = bpe.encode(data.feature_string.values.tolist(), output_type=yttm.OutputType.ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(f) for f in features_ids)\n",
    "\n",
    "features_ids = [ f[:max_len] + [0]*(max_len-len(f)) for f in features_ids ]\n",
    "features_ids = np.array(features_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = TensorDataset(torch.tensor(features_ids,dtype=int),\n",
    "#                         torch.tensor(data[\"feature_float\"].values.reshape(-1,1),dtype=torch.float),\n",
    "#                         torch.tensor(data[\"label\"].values,dtype=int),\n",
    "#                         torch.tensor(data[\"weights\"].values,dtype=torch.float))\n",
    "# ntrain = int(.75*len(dataset))\n",
    "# dataset_train, dataset_test = random_split(dataset,[ntrain, len(dataset)-ntrain])\n",
    "# train_dataloader = DataLoader(dataset,batch_size=64,shuffle=True)\n",
    "# test_dataloader = DataLoader(dataset,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6703856415231628"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-data[\"label\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def acc(y,logits):\n",
    "    yhat = logits.argmax(dim=1)\n",
    "    return (1.0*(yhat==y)).mean()\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "class TransClassifier(pl.LightningModule):\n",
    "    def __init__(self, h = 32, batch_size = 64, dropout_rate = .2, num_emb = 1000, seq_type = None):\n",
    "        super().__init__()\n",
    "        self.h = h\n",
    "        self.batch_size = batch_size\n",
    "        self.emb = torch.nn.Embedding(num_emb, embedding_dim=h,padding_idx=0)\n",
    "        \n",
    "        if seq_type==\"cnn\":\n",
    "            self.seq_encoder = torch.nn.Conv1d(h,h,3)\n",
    "        elif seq_type=\"lstm\":\n",
    "            self.seq_encoder = torch.nn.LSTM(h,h,batch_first=True)\n",
    "        else:\n",
    "            self.seq_encoder = None\n",
    "            \n",
    "        self.lin1 = torch.nn.Linear(h+1,h+1)\n",
    "        self.cls = torch.nn.Linear(h+1,2)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.drop = torch.nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def input_drop_out(self,x):\n",
    "        if self.training:\n",
    "            to_zero = torch.rand(*x.shape)<=self.dropout_rate\n",
    "            x[to_zero] = 0\n",
    "        return x\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x_s,x_f,y=None,w = None):\n",
    "        \n",
    "        x_s = self.input_drop_out(x_s)\n",
    "        \n",
    "        x_emb = self.emb(x_s)\n",
    "        \n",
    "        x = self.cnn(x_emb.permute(0,2,1))#.permute(0,1,2)\n",
    "\n",
    "        x_reduced,_ = x.max(dim=2)\n",
    "        \n",
    "        x_comb = torch.cat([x_reduced, x_f], dim = 1)\n",
    "        x_comb = self.drop(x_comb)\n",
    "#         x_comb = self.lin1(x_comb)\n",
    "        x_comb = torch.nn.functional.relu(x_comb)\n",
    "        logits = self.cls(x_comb)\n",
    "        \n",
    "        out = (logits,)\n",
    "        \n",
    "        if y is not None:\n",
    "            temp = functional.cross_entropy(logits,y,reduction='none')\n",
    "            if w is not None:\n",
    "                loss = (temp*w).mean()\n",
    "            else:\n",
    "                loss = temp.mean()\n",
    "            \n",
    "            out = out + (loss, acc(y,logits))\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "    \n",
    "    def _step(self, batch, batch_idx):\n",
    "        x_s,x_f,y,w = batch\n",
    "        yhat, l, a = self.forward(x_s,x_f,y,w)\n",
    "        return dict(loss = l, acc = a)\n",
    "                                \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        out_dict = self._step(batch, batch_idx)\n",
    "        return dict(loss = out_dict[\"loss\"],log = out_dict)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        out_dict = self._step(batch, batch_idx)\n",
    "        return {f'val_{k}':v for k,v in out_dict.items()}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        temp = defaultdict(list)\n",
    "        for output in outputs:\n",
    "            for k,v in output.items():\n",
    "                temp[k].append(v)\n",
    "                \n",
    "        for k,v in temp.items():\n",
    "            temp[k] = torch.stack(v).mean()\n",
    "            \n",
    "        temp = {k:v for k,v in temp.items()}\n",
    "                \n",
    "        temp[\"log\"] = {k:v for k,v in temp.items()}\n",
    "        return temp\n",
    "        \n",
    "        \n",
    "    def prepare_data(self):\n",
    "        dataset = TensorDataset(torch.tensor(features_ids,dtype=int),\n",
    "                        torch.tensor(data[\"feature_float\"].values.reshape(-1,1),dtype=torch.float),\n",
    "                        torch.tensor(data[\"label\"].values,dtype=int),\n",
    "                        torch.tensor(data[\"weights\"].values,dtype=torch.float))\n",
    "        ntrain = int(.75*len(dataset))\n",
    "        dataset_train, dataset_val = random_split(dataset,[ntrain, len(dataset)-ntrain])\n",
    "        self.dataset_train = dataset\n",
    "        self.dataset_val = dataset_val\n",
    "        \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset_train,batch_size=self.batch_size,shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.dataset_val,batch_size=self.batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "        \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning:\n",
      "  | Name | Type      | Params\n",
      "-------------------------------\n",
      "0 | emb  | Embedding | 512 K \n",
      "1 | cnn  | Conv1d    | 786 K \n",
      "2 | lin1 | Linear    | 263 K \n",
      "3 | cls  | Linear    | 1 K   \n",
      "4 | drop | Dropout   | 0     \n",
      "/Users/kirill.trapeznikov/miniconda3/envs/sabi/lib/python3.7/site-packages/pytorch_lightning/utilities/warnings.py:18: RuntimeWarning: Displayed epoch numbers in the progress bar start from \"1\" until v0.6.x, but will start from \"0\" in v0.8.0.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/Users/kirill.trapeznikov/miniconda3/envs/sabi/lib/python3.7/site-packages/pytorch_lightning/utilities/warnings.py:18: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "INFO:lightning:Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransClassifier(dropout_rate=0, h = 512, batch_size=256)\n",
    "trainer = pl.Trainer(max_epochs=300,\n",
    "                     min_epochs=10,\n",
    "#                      early_stop_callback = pl.callbacks.EarlyStopping(monitor=\"val_acc\", patience = 10, verbose=True),\n",
    "                    progress_bar_refresh_rate = 0)\n",
    "trainer.fit(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sabi",
   "language": "python",
   "name": "sabi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
